{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eddb0a6-27ba-49eb-8b21-298d27b1a498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 02: Preprocessing the Medical FAQ Dataset\n",
    "This notebook focuses on preparing the data for the upcoming LLM fine-tuning stage. I start with the cleaned and transformed dataset from the previous step, which is stored in Azure Blob Storage as a Parquet file. The goal here is to refine the text further so that it is ready for training a model like DistilGPT-2. This preparation makes the data more consistent and meaningful, which directly supports the project's aim of creating accurate, multilingual responses to patient questions in telehealth. By doing this, I ensure the final tool can reduce staff workload in healthcare settings, where efficient data handling can lead to real cost savings and better patient care.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d6c598-cd6d-415e-b0e4-bcd993d42135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/godhanaravara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/godhanaravara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/godhanaravara/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for Text preprocessing, tokenization, and lemmatization\n",
    "import nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9bc6834-df0b-4012-977c-2691daa3da7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Loading the saved Parquet file\n",
    "\n",
    "I initiate this step to retrieve the cleaned dataset I previously saved as transformed_medquad.parquet from Azure Blob Storage and prepare the Natural Language Toolkit (NLTK) for text processing. This dataset contains 16,359 rows of medical FAQ data with tranformed columns, ready for advanced refinement. \n",
    "- I use Spark to read the Parquet file from the mounted path /mnt/AZ_CONTAINER/transformed_medquad.parquet and display its row count, schema, and first five rows to verify the data. Additionally, I downloaded NLTK resources (punkt, stopwords, wordnet) to enable tokenization, stopword removal, and lemmatization.\n",
    "- The purpose is to pick up where I left off, avoiding the need to repeat basic cleaning every time. I did it this way because Parquet files are efficient for storage and quick to load in Spark, which fits my project's use of Databricks for distributed processing. \n",
    "- This helps the problem statement by keeping the workflow smooth and scalable, allowing me to handle larger datasets in the future without performance issues. From a business perspective, it shows how cloud storage integrates with processing tools to cut down on time and resources. This sets the stage for text analysis, supporting the business goal of delivering accurate healthcare FAQs, which can save healthcare providers significant costs (up to 60% versus on-premises solutions, per Azure economics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5552bc-17c0-466e-a4c4-c23f3c43e661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Parquet with 16359 rows\n",
      "First 5 rows:\n",
      "+-----------------------------------+-----------------------------------+-----------------+-----------------------------------+\n",
      "|                 transform_question|                             answer|           source|                         focus_area|\n",
      "+-----------------------------------+-----------------------------------+-----------------+-----------------------------------+\n",
      "|what are the symptoms of periphe...|People who have P.A.D. may have ...|  NIHSeniorHealth|Peripheral Arterial Disease (P.A...|\n",
      "|who is at risk for adult acute m...|Smoking, previous chemotherapy t...|        CancerGov|       Adult Acute Myeloid Leukemia|\n",
      "|what are the treatments for myel...|Because myelodysplastic /myelopr...|        CancerGov|Myelodysplastic/ Myeloproliferat...|\n",
      "|what research or clinical trials...|New types of treatment are being...|        CancerGov|                        Skin Cancer|\n",
      "|   what is are childbirth problems |While childbirth usually goes we...|MPlusHealthTopics|                Childbirth Problems|\n",
      "+-----------------------------------+-----------------------------------+-----------------+-----------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Load the saved parquet file from Azure Blob\n",
    "df = spark.read.parquet(\"/mnt/faqdata/transformed_medquad.parquet\")\n",
    "print(f\"Loaded Parquet with {df.count()} rows\")\n",
    "\n",
    "print(\"First 5 rows:\")\n",
    "df.show(5, truncate=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0772d3d6-7081-4c84-9ccb-eac4673f52a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transform_question: string (nullable = true)\n",
      " |-- answer: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- focus_area: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f40f9ddd-c488-4c35-8249-923e836a09a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Tokenizing the Text\n",
    "\n",
    "I create a user-defined function (UDF) to tokenize the `transform_question` and `transform_answer` columns, turning them into lists of words. The purpose is to split sentences into individual parts, making it easier to filter and format it suitable for deeper linguistic analysis. \n",
    "- I implement it as a UDF in Spark using NLTK's `word_tokenize` because it allows distributed processing across the dataset's 16,359 rows. This way supports the project's need for handling real-world medical questions efficiently.\n",
    "- This prepares the data, by generating new columns `token_question` and `token_answer`, for more advanced filtering, which ultimately leads to better model performance in generating helpful telehealth answers. \n",
    "- This improves FAQ response quality with structured data handling and enhances patient education.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8394118b-f79b-42e2-9b11-749daca07f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized row count: 16359\n",
      "First 5 rows after tokenization:\n",
      "+-----------------+-----------------------------------+-----------------------------------+-----------------------------------+\n",
      "|           source|                         focus_area|                     token_question|                    original_answer|\n",
      "+-----------------+-----------------------------------+-----------------------------------+-----------------------------------+\n",
      "|  NIHSeniorHealth|Peripheral Arterial Disease (P.A...|[what, are, the, symptoms, of, p...|People who have P.A.D. may have ...|\n",
      "|        CancerGov|       Adult Acute Myeloid Leukemia|[who, is, at, risk, for, adult, ...|Smoking, previous chemotherapy t...|\n",
      "|        CancerGov|Myelodysplastic/ Myeloproliferat...|[what, are, the, treatments, for...|Because myelodysplastic /myelopr...|\n",
      "|        CancerGov|                        Skin Cancer|[what, research, or, clinical, t...|New types of treatment are being...|\n",
      "|MPlusHealthTopics|                Childbirth Problems|[what, is, are, childbirth, prob...|While childbirth usually goes we...|\n",
      "+-----------------+-----------------------------------+-----------------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Define tokenization\n",
    "def tokenize_text(text):\n",
    "    if text:\n",
    "        return nltk.word_tokenize(text)\n",
    "    return []\n",
    "\n",
    "tokenize_udf = udf(tokenize_text, ArrayType(StringType()))\n",
    "\n",
    "# Apply tokenization\n",
    "try:\n",
    "    df_tokenized = df.select(\n",
    "        \"source\",\n",
    "        \"focus_area\",\n",
    "        tokenize_udf(df.transform_question).alias(\"token_question\"),\n",
    "        col(\"answer\").alias(\"original_answer\")\n",
    "    )\n",
    "    print(f\"Tokenized row count: {df_tokenized.count()}\")\n",
    "    print(\"First 5 rows after tokenization:\")\n",
    "    df_tokenized.show(5, truncate=35)\n",
    "except Exception as err:\n",
    "    print(\"Error tokenizing text:\", err)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c62b034-47bd-488f-8fd2-60d6729e8fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Removing Stopwords\n",
    "\n",
    "Using another UDF, I filter out common English stopwords (like \"the\", \"is\", etc.) from the tokenized columns to focus on the significant key terms. \n",
    "-  I utilize NLTK's stopword list for this because it is reliable, has a broad coverage of English, and is built-in which avoids extra dependencies in my setup. This creates `clean_question` and `clean_answer` columns.\n",
    "- This step of the project reduces the data noise, improving the LLM's focus on relevant medical content. It helps in potentially lowering error rates in FAQ responses and saves time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97820ee8-6829-45ac-bc07-184aafa9e15a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row count: 16359\n",
      "First 5 rows after stopwords removal:\n",
      "+-----------------+-----------------------------------+-----------------------------------+-----------------------------------+\n",
      "|           source|                         focus_area|                     clean_question|                    original_answer|\n",
      "+-----------------+-----------------------------------+-----------------------------------+-----------------------------------+\n",
      "|  NIHSeniorHealth|Peripheral Arterial Disease (P.A...|[symptoms, peripheral, arterial,...|People who have P.A.D. may have ...|\n",
      "|        CancerGov|       Adult Acute Myeloid Leukemia|[risk, adult, acute, myeloid, le...|Smoking, previous chemotherapy t...|\n",
      "|        CancerGov|Myelodysplastic/ Myeloproliferat...|[treatments, myelodysplastic, my...|Because myelodysplastic /myelopr...|\n",
      "|        CancerGov|                        Skin Cancer|[research, clinical, trials, don...|New types of treatment are being...|\n",
      "|MPlusHealthTopics|                Childbirth Problems|             [childbirth, problems]|While childbirth usually goes we...|\n",
      "+-----------------+-----------------------------------+-----------------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define UDF to remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    if tokens:\n",
    "        return [word for word in tokens if word.lower() not in stop_words]\n",
    "    return []\n",
    "\n",
    "remove_stopwords_udf = udf(remove_stopwords, ArrayType(StringType()))\n",
    "\n",
    "# Apply stopwords removal\n",
    "try:\n",
    "    df_processed = df_tokenized.select(\n",
    "        \"source\",\n",
    "        \"focus_area\",\n",
    "        remove_stopwords_udf(df_tokenized.token_question).alias(\"clean_question\"),\n",
    "        col(\"original_answer\")\n",
    "    )\n",
    "    print(f\"Processed row count: {df_processed.count()}\")\n",
    "    print(\"First 5 rows after stopwords removal:\")\n",
    "    df_processed.show(5, truncate=35)\n",
    "except Exception as err:\n",
    "    print(\"Error removing stopwords:\", err)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc50c804-430f-4550-bcd3-31e1e41b88c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Lemmatizing the text\n",
    "\n",
    "I apply a lemmatization UDF to reduce words to their base form, like changing \"running\" to \"run\". This is to standardize variations, ensuring the model treats similar words as one. \n",
    "- I use NLTK's `WordNetLemmatizer` because it integrates well with the tokenization step and keeps the code simple. I chose this technique, over stemming to preserve word meaning, and enhance data consistency which is vital for training an LLM on medical FAQs where context matters and improves accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f5a4c6-4e04-455f-b411-f89568dae741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized row count: 16359\n",
      "First 5 rows after lemmatization:\n",
      "+-----------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|           source|                              focus_area|                          lemma_question|                         original_answer|\n",
      "+-----------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|  NIHSeniorHealth|    Peripheral Arterial Disease (P.A.D.)|[symptom, peripheral, arterial, disea...|People who have P.A.D. may have sympt...|\n",
      "|        CancerGov|            Adult Acute Myeloid Leukemia| [risk, adult, acute, myeloid, leukemia]|Smoking, previous chemotherapy treatm...|\n",
      "|        CancerGov|Myelodysplastic/ Myeloproliferative N...|[treatment, myelodysplastic, myelopro...|Because myelodysplastic /myeloprolife...|\n",
      "|        CancerGov|                             Skin Cancer|[research, clinical, trial, done, ski...|New types of treatment are being test...|\n",
      "|MPlusHealthTopics|                     Childbirth Problems|                   [childbirth, problem]|While childbirth usually goes well, c...|\n",
      "+-----------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define lemmatization\n",
    "def lemmatize_tokens(tokens):\n",
    "    if tokens:\n",
    "        return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return []\n",
    "\n",
    "lemmatize_udf = udf(lemmatize_tokens, ArrayType(StringType()))\n",
    "\n",
    "# Apply lemmatization\n",
    "try:\n",
    "    df_lemmatized = df_processed.select(\n",
    "        \"source\",\n",
    "        \"focus_area\",\n",
    "        lemmatize_udf(df_processed.clean_question).alias(\"lemma_question\"),\n",
    "        col(\"original_answer\")\n",
    "    )\n",
    "    print(f\"Lemmatized row count: {df_lemmatized.count()}\")\n",
    "    print(\"First 5 rows after lemmatization:\")\n",
    "    df_lemmatized.show(5, truncate=40)\n",
    "except Exception as err:\n",
    "    print(\"Error lemmatizing text:\", err)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b819742d-4c1b-496c-89a5-10f204c01b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Checking for Bias in `focus_area`\n",
    "\n",
    "I group the data by `focus_area` to assess and review its distribution of values and calculate percentages, flagging if any category dominates over 50% that could skew the dataset or model outcomes. \n",
    "- The purpose is to identify potential bias early and the dataset represents a broad range of medical topics. \n",
    "- I do this with Spark's `groupBy` and `filter` functions because it is efficient for the dataset's size. This step is done to promote fairness in the project, avoiding skewed LLM outputs. \n",
    "- This approach supports equitable telehealth responses which is a key aspect of healthcare access and balances computational ease with meaningful insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcba4a80-f12f-48a2-9fe4-5b50e5e64b9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus area distribution:\n",
      "No bias detected\n"
     ]
    }
   ],
   "source": [
    "focus_dist = df_lemmatized.groupBy(\"focus_area\").count().orderBy(\"count\", ascending=False)\n",
    "print(\"Focus area distribution:\")\n",
    "\n",
    "# Basic bias check: if one category dominates (>50% of rows)\n",
    "total_rows = df_lemmatized.count()\n",
    "focus_dist = focus_dist.withColumn(\"percent\", (col(\"count\") / total_rows * 100))\n",
    "dominant = focus_dist.filter(col(\"percent\") > 50).count()\n",
    "if dominant > 0:\n",
    "    print(\"Warning: Potential bias detected - one focus_area exceeds 50% of data.\")\n",
    "else:\n",
    "    print(\"No bias detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0889e7d8-010c-45cc-830a-884d825ae641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Saving the preprocessed data\n",
    "\n",
    "I save the lemmatized DataFrame back to Azure Blob as 'preprocessed_medquad.parquet' to maintain the refined data for future stages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9347f748-80af-4525-8012-a9d6e368003f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to storage blob\n",
      "Verified saved data with 16359 rows\n",
      "+-----------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|           source|                              focus_area|                          lemma_question|                         original_answer|\n",
      "+-----------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "|  NIHSeniorHealth|    Peripheral Arterial Disease (P.A.D.)|[symptom, peripheral, arterial, disea...|People who have P.A.D. may have sympt...|\n",
      "|        CancerGov|            Adult Acute Myeloid Leukemia| [risk, adult, acute, myeloid, leukemia]|Smoking, previous chemotherapy treatm...|\n",
      "|        CancerGov|Myelodysplastic/ Myeloproliferative N...|[treatment, myelodysplastic, myelopro...|Because myelodysplastic /myeloprolife...|\n",
      "|        CancerGov|                             Skin Cancer|[research, clinical, trial, done, ski...|New types of treatment are being test...|\n",
      "|MPlusHealthTopics|                     Childbirth Problems|                   [childbirth, problem]|While childbirth usually goes well, c...|\n",
      "+-----------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Saving the preprocessed data to Storage Blob\n",
    "output_path = \"f{MOUNT_PT}/preprocessed_medquad.parquet\"\n",
    "\n",
    "try:\n",
    "    df_lemmatized.write.mode(\"overwrite\").parquet(output_path)\n",
    "    print(f\"Preprocessed data saved to storage blob\")\n",
    "\n",
    "    # Verifying the save to ensure the data was saved completely\n",
    "    df_verified = spark.read.parquet(output_path)\n",
    "    print(f\"Verified saved data with {df_verified.count()} rows\")\n",
    "    df_verified.show(5, truncate=35)\n",
    "except Exception as err:\n",
    "    print(\"Error saving data:\", err)\n",
    "    print(\"Check SAS Token expiry and 'Write' permission\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "This notebook successfully preprocessed the transformed Medical FAQ dataset (`transformed_medquad.parquet`, 16,359 rows) from Azure Blob Storage, applying lemmatization to questions using NLTK and Spark UDFs, checking for bias in focus_area, and saving the refined data as `preprocessed_medquad.parquet` for downstream fine-tuning and RAG in the Healthcare FAQ Generator pipeline.\n",
    "\n",
    "### Key Results\n",
    "\n",
    "- Row Count: 16,359 rows preserved after lemmatization on 'question' column only.\n",
    "- Lemmatization: Questions tokenized, lemmatized, and stored as arrays (e.g., \"symptom peripheral arterial disease pad\" â†’ **[symptom, peripheral, arterial, disease, pad]**).\n",
    "- Bias Check: No dominant focus_area (>50%), indicating balanced representation across medical topics.\n",
    "- Runtime: ~5-10 minutes on Databricks Community Edition (CPU).\n",
    "- Output: preprocessed_medquad.parquet with columns `source, focus_area, lemma_question (array), original_answer (string)`.\n",
    "\n",
    "### Business Impact\n",
    "\n",
    "- Prepares consistent, meaningful text data for multilingual FAQ generation, enabling accurate telehealth responses and reducing clinician workload.\n",
    "- Promotes fairness by mitigating bias in focus areas, ensuring equitable healthcare information access.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Model Fine-Tuning: Use the preprocessed Parquet to fine-tune flan-t5-base for FAQ generation in the next notebook (03_train_LLM.ipynb).\n",
    "- RAG Pipeline: Build retrieval and evaluation with LangChain and FAISS.\n",
    "- Multilingual Expansion: Integrate GCP Translation API for Spanish/Telugu support.\n",
    "\n",
    "This step demonstrates scalable NLP preprocessing with PySpark and NLTK, forming a strong foundation for healthcare applications."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_preprocess_medquad",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python (projectML)",
   "language": "python",
   "name": "projectml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
