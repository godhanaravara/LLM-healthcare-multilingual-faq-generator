{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Load and Clean Data\n",
    "## Overview\n",
    "This notebook ingests the raw Medical FAQ dataset (`medquad.csv`), performs foundational cleaning (e.g., text transformation), and persists the data in Parquet format for efficient downstream processing in the Healthcare FAQ Generator pipeline. It uses PySpark on Databricks for scalable data handling and mounts Azure Blob Storage for secure access.\n",
    "\n",
    "## **Purpose**  \n",
    "- Load raw CSV data from Azure Blob Storage.\n",
    "- Apply basic transformations (lowercase, remove special characters).\n",
    "- Save as Parquet for fast, columnar storage and analytics.\n",
    "\n",
    "### **Why this step matters**  \n",
    "* Establishes a reliable, repeatable starting point for the pipeline.  \n",
    "* Converts slow CSV reads into fast, analytics-friendly Parquet for both Spark and Python.  \n",
    "* Catches early data issues before they propagate into training and evaluation.\n",
    "\n",
    "### **Business value**  \n",
    "* Faster experimentation cycles for model training and retrieval indexing.  \n",
    "* Lower storage and compute costs due to columnar compression and predicate pushdown.  \n",
    "* Traceable data lineage from raw to curated, supporting auditability and handoffs.\n",
    "\n",
    "### **Technical Approach**  \n",
    "- **Input**: `medquad.csv` (16,412 rows) from Azure Blob.\n",
    "- **Output**: `transformed_medquad.parquet` (16,359 rows after cleaning).\n",
    "- **Tools**: PySpark for distributed processing, Azure Blob mounting with SAS token.\n",
    "- **Runtime**: ~1-2 minutes on Databricks Community Edition (CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6f1eeb-517e-45a9-9a33-9da868ac51c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, count, when, lower, regexp_replace\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Configure environment and access\n",
    "\n",
    "### **Purpose**  \n",
    "Load configuration from `.env` and mount Azure Blob Storage for secure data access, ensuring least-privilege access without hardcoding secrets.\n",
    "\n",
    "### Technical Details\n",
    "- Uses `dotenv` to load SAS token, account, container, and mount point.\n",
    "- Mounts `wasbs://container@account.blob.core.windows.net` to Blob container.\n",
    "- Checks existing mounts to avoid remounting unnecessarily.\n",
    "- SAS token provides read/write access with expiration.\n",
    "\n",
    "### **Acceptance checks**  \n",
    "*  `.env` or secret scope is available  \n",
    "* Required keys present: storage account, container, mount path, SAS key  \n",
    "* Spark session (if used) is initialized\n",
    "\n",
    "### **Risks and mitigation**  \n",
    "* Missing credentials → clear error with instructions  \n",
    "* Wrong container or path → fail fast with a message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "924fadd3-db70-48af-baeb-2eb82dcc7980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Mount Azure Blob container to DBFS using SAS token\n",
    "\n",
    "SAS_TOKEN = os.getenv(\"SAS_TOKEN\") \n",
    "ACCOUNT   = os.getenv(\"STORAGE_ACC_NAME\")\n",
    "CONTAINER = os.getenv(\"CONT_NAME\")\n",
    "MOUNT_PT  = os.getenv(\"MOUNT_PT\")\n",
    "\n",
    "SOURCE    = f\"wasbs://{CONTAINER}@{ACCOUNT}.blob.core.windows.net\"\n",
    "CFGKEY    = f\"fs.azure.sas.{CONTAINER}.{ACCOUNT}.blob.core.windows.net\"\n",
    "\n",
    "\n",
    "# current mounts as {mountPoint: SOURCE}\n",
    "mounted = {m.mountPoint: m.source for m in dbutils.fs.mounts()}\n",
    "\n",
    "try:\n",
    "    if MOUNT_PT in mounted:\n",
    "        # already mounted\n",
    "        if mounted[MOUNT_PT] != SOURCE:\n",
    "            # mounted to something else -> fix it\n",
    "            dbutils.fs.unmount(MOUNT_PT)\n",
    "            dbutils.fs.mount(source=SOURCE, mount_point=MOUNT_PT, extra_configs={CFGKEY: SAS_TOKEN})\n",
    "    else:\n",
    "        # not mounted yet\n",
    "        dbutils.fs.mount(source=SOURCE, mount_point=MOUNT_PT, extra_configs={CFGKEY: SAS_TOKEN})\n",
    "except Exception:\n",
    "    # keep it quiet but re-raise so failures surface when needed\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Raw Dataset\n",
    "\n",
    "### Purpose\n",
    "Read the raw `medquad.csv` file from Azure Blob using PySpark, preserving original schema and handling multiline/quoted fields.\n",
    "\n",
    "### Insights:\n",
    "- Ingests 16,412 raw medical FAQ records efficiently, forming the foundation for FAQ generation.\n",
    "- Handles large datasets scalably, supporting healthcare data pipelines.\n",
    "\n",
    "### Technical Details\n",
    "- Options: `header=true`, `multiLine=true`, `escape=\"\\\"\"`, `quote=\"\\\"\"` for CSV parsing.\n",
    "- Path: `CONTAINER/medquad.csv` (mounted Azure Blob).\n",
    "- Output: Spark DataFrame with columns `question`, `answer`, `source`, `focus_area`.\n",
    "- Row count: 16,412 (verified with `df.count()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded row count: 16412\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[question: string, answer: string, source: string, focus_area: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+------------------------------------------------------------+---------------+----------+\n",
      "|                              question|                                                      answer|         source|focus_area|\n",
      "+--------------------------------------+------------------------------------------------------------+---------------+----------+\n",
      "|              What is (are) Glaucoma ?|Glaucoma is a group of diseases that can damage the eye's...|NIHSeniorHealth|  Glaucoma|\n",
      "|                What causes Glaucoma ?|Nearly 2.7 million people have glaucoma, a leading cause ...|NIHSeniorHealth|  Glaucoma|\n",
      "|   What are the symptoms of Glaucoma ?|Symptoms of Glaucoma  Glaucoma can develop in one or both...|NIHSeniorHealth|  Glaucoma|\n",
      "|What are the treatments for Glaucoma ?|Although open-angle glaucoma cannot be cured, it can usua...|NIHSeniorHealth|  Glaucoma|\n",
      "|              What is (are) Glaucoma ?|Glaucoma is a group of diseases that can damage the eye's...|NIHSeniorHealth|  Glaucoma|\n",
      "+--------------------------------------+------------------------------------------------------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Intitialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"LoadMedQuAD\").getOrCreate()\n",
    "data_path = f\"/dbfs{os.getenv('MOUNT_PT')}/medquad.csv\"\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "                        .option(\"multiLine\", \"true\") \\\n",
    "                        .option(\"escape\", \"\\\"\") \\\n",
    "                        .option(\"quote\", \"\\\"\") \\\n",
    "                        .csv(data_path)\n",
    "print(f\"Reloaded row count: {df.count()}\")\n",
    "print(\"First 5 rows:\")\n",
    "display(df.limit(5))\n",
    "df.show(5, truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "732794fd-2e45-4339-adb8-f23935216ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- question: string (nullable = true)\n",
      " |-- answer: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- focus_area: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the schema to understand column names and types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Handling Duplicates, missing values and text transformation\n",
    "\n",
    "### **Purpose**  \n",
    "Apply basic cleaning to the raw dataset (lowercase questions, remove special characters) to prepare for downstream processing like lemmatization and model training.\n",
    "\n",
    "### **What to check**  \n",
    "* Null distribution per column  \n",
    "* Sample records for visual inspection  \n",
    "* Duplicate headline counts by key fields\n",
    "\n",
    "### **Why?**  \n",
    "- Standardizes text data, improving model performance and FAQ relevance in telehealth applications.\n",
    "- Reduces noise (e.g., punctuation), enabling better tokenization and similarity scoring.\n",
    "\n",
    "### **Technical Details**  \n",
    "- Uses `lower` and `regexp_replace` to clean `question` column (alphanumeric and spaces only).\n",
    "- Retains `answer`, `source`, `focus_area` unchanged.\n",
    "- Output: `transformed_df` with 16,359 rows (after any null filtering if applied).\n",
    "- Schema: `transform_question` (string), `answer` (string), `source` (string), `focus_area` (string).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates. New row count: 16364\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows based on question and answer\n",
    "df_no_dup = df.dropDuplicates([\"question\", \"answer\", \"source\", \"focus_area\"])\n",
    "print(f\"Removed duplicates. New row count: {df_no_dup.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b90d3d5-b471-4b9a-a356-ab400d11ea50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- After removing duplicates: 16364\n",
      "Rows with null 'answers' after removing duplicates: 5\n",
      "\n",
      "Cleaning NULL values in 'answers'...\n",
      "\n",
      "Rows with null 'answers' after cleaning: 0\n",
      "--Remaining rows in the dataset: 16359\n"
     ]
    }
   ],
   "source": [
    "row_count_bef = df_no_dup.count()\n",
    "print(f\"-- After removing duplicates: {row_count_bef}\")\n",
    "null_count_bef = df_no_dup.filter(col(\"answer\").isNull()).count()\n",
    "print(f\"Rows with null 'answers' after removing duplicates: {null_count_bef}\\n\")\n",
    "\n",
    "# Removing NULL 'answers'\n",
    "df_cleaned = df_no_dup.filter(df_no_dup.answer.isNotNull())\n",
    "print(\"Cleaning NULL values in 'answers'...\\n\")\n",
    "\n",
    "null_count = df_cleaned.filter(col(\"answer\").isNull()).count()\n",
    "print(f\"Rows with null 'answers' after cleaning: {null_count}\")\n",
    "row_count = df_cleaned.count()\n",
    "print(f\"--Remaining rows in the dataset: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "383bdead-0709-4219-97c8-68be1c5759b6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756526569286}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------+-----------------+-----------------------------------+\n",
      "|                           question|                             answer|           source|                         focus_area|\n",
      "+-----------------------------------+-----------------------------------+-----------------+-----------------------------------+\n",
      "|What are the symptoms of Periphe...|People who have P.A.D. may have ...|  NIHSeniorHealth|Peripheral Arterial Disease (P.A...|\n",
      "|Who is at risk for Adult Acute M...|Smoking, previous chemotherapy t...|        CancerGov|       Adult Acute Myeloid Leukemia|\n",
      "|What are the treatments for Myel...|Because myelodysplastic /myelopr...|        CancerGov|Myelodysplastic/ Myeloproliferat...|\n",
      "|what research (or clinical trial...|New types of treatment are being...|        CancerGov|                        Skin Cancer|\n",
      "|What is (are) Childbirth Problems ?|While childbirth usually goes we...|MPlusHealthTopics|                Childbirth Problems|\n",
      "+-----------------------------------+-----------------------------------+-----------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display(df_cleaned.limit(5))\n",
    "df_cleaned.show(5, truncate=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ea506c-deb0-4c6b-a50e-60361fd9269e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756526592477}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------+-----------------+-----------------------------------+\n",
      "|                 transform_question|                             answer|           source|                         focus_area|\n",
      "+-----------------------------------+-----------------------------------+-----------------+-----------------------------------+\n",
      "|what are the symptoms of periphe...|People who have P.A.D. may have ...|  NIHSeniorHealth|Peripheral Arterial Disease (P.A...|\n",
      "|who is at risk for adult acute m...|Smoking, previous chemotherapy t...|        CancerGov|       Adult Acute Myeloid Leukemia|\n",
      "|what are the treatments for myel...|Because myelodysplastic /myelopr...|        CancerGov|Myelodysplastic/ Myeloproliferat...|\n",
      "|what research or clinical trials...|New types of treatment are being...|        CancerGov|                        Skin Cancer|\n",
      "|   what is are childbirth problems |While childbirth usually goes we...|MPlusHealthTopics|                Childbirth Problems|\n",
      "+-----------------------------------+-----------------------------------+-----------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform text: lowercase and remove special characters\n",
    "\n",
    "transformed_df = df_cleaned.select(\n",
    "    lower(regexp_replace(col(\"question\"), \"[^a-zA-Z0-9\\\\s]\", \"\")).alias(\"transform_question\"),\n",
    "    # Retaining 'answer','source' and 'focus_area' columns\n",
    "    col(\"answer\"),\n",
    "    col(\"source\"),\n",
    "    col(\"focus_area\")\n",
    ")\n",
    "\n",
    "# display(transformed_df.limit(5))\n",
    "transformed_df.show(5, truncate=35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Helper Functions for Persistence\n",
    "\n",
    "### Purpose\n",
    "Defines utility functions to ensure reliable Parquet saving, including mount checks and overwrite logic, for reproducible data pipelines.\n",
    "\n",
    "- Ensures data persistence is idempotent and fault-tolerant, supporting iterative development in healthcare data workflows.\n",
    "- Optimizes storage with Parquet compression and coalescing for faster reads.\n",
    "\n",
    "### Technical Details\n",
    "- `ensure_mount`: Mounts Azure Blob if not already done.\n",
    "- `path_exists`/`is_valid_parquet`: Checks file validity.\n",
    "- `ensure_parquet`: Writes Parquet if needed (new, recovered, skipped, or overwritten based on row count).\n",
    "- Output: `transformed_medquad.parquet` in Blob container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ac66cfd-f909-4c20-b67c-1606ea5c3479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- helpers\n",
    "def ensure_mount(mount_pt=MOUNT_PT, source=SOURCE, cfgkey=CFGKEY, sas=SAS_TOKEN):\n",
    "    \"\"\"Mount once. If already mounted to the right source, do nothing.\"\"\"\n",
    "    mounted = {m.mountPoint: m.source for m in dbutils.fs.mounts()}\n",
    "    if mount_pt not in mounted:\n",
    "        dbutils.fs.mount(source=source, mount_point=mount_pt, extra_configs={cfgkey: sas})\n",
    "        return \"mounted\"\n",
    "    if mounted[mount_pt] != source:\n",
    "        dbutils.fs.unmount(mount_pt)\n",
    "        dbutils.fs.mount(source=source, mount_point=mount_pt, extra_configs={cfgkey: sas})\n",
    "        return \"remounted\"\n",
    "    return \"ok\"\n",
    "\n",
    "def path_exists(path):\n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def is_valid_parquet(path):\n",
    "    try:\n",
    "        _ = spark.read.parquet(path).limit(1).count()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def ensure_parquet(df, out_path, coalesce_one=True):\n",
    "    \"\"\"\n",
    "    Write df to out_path if needed:\n",
    "      - If out_path missing -> write\n",
    "      - If present but unreadable -> clean + write\n",
    "      - If present and row count matches -> skip\n",
    "      - Else overwrite\n",
    "    \"\"\"\n",
    "    if not path_exists(out_path):\n",
    "        (df.coalesce(1) if coalesce_one else df).write.mode(\"overwrite\").parquet(out_path)\n",
    "        return \"written (new)\"\n",
    "\n",
    "    if not is_valid_parquet(out_path):\n",
    "        dbutils.fs.rm(out_path, recurse=True)\n",
    "        (df.coalesce(1) if coalesce_one else df).write.mode(\"overwrite\").parquet(out_path)\n",
    "        return \"written (recovered)\"\n",
    "\n",
    "    cur = spark.read.parquet(out_path)\n",
    "    if df.count() == cur.count():\n",
    "        return \"skipped (same row count)\"\n",
    "    # overwrite if different size; requires 'd' in SAS\n",
    "    (df.coalesce(1) if coalesce_one else df).write.mode(\"overwrite\").parquet(out_path)\n",
    "    return \"written (overwrote)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58bdd54a-ada0-49f9-8691-69381e108afd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mount: ok\n",
      "write: written (new)\n",
      "9925038\n",
      "rows: 16359\n",
      "+-----------------------------------+-----------------------------------+---------------+-----------------------------------+\n",
      "|                 transform_question|                             answer|         source|                         focus_area|\n",
      "+-----------------------------------+-----------------------------------+---------------+-----------------------------------+\n",
      "|what are the symptoms of periphe...|People who have P.A.D. may have ...|NIHSeniorHealth|Peripheral Arterial Disease (P.A...|\n",
      "|who is at risk for adult acute m...|Smoking, previous chemotherapy t...|      CancerGov|       Adult Acute Myeloid Leukemia|\n",
      "|what are the treatments for myel...|Because myelodysplastic /myelopr...|      CancerGov|Myelodysplastic/ Myeloproliferat...|\n",
      "+-----------------------------------+-----------------------------------+---------------+-----------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# --- use them with transformed df\n",
    "status = ensure_mount()\n",
    "print(\"mount:\", status)\n",
    "\n",
    "OUT_PATH = f\"{MOUNT_PT}/transformed_medquad.parquet\"\n",
    "result = ensure_parquet(transformed_df, OUT_PATH, coalesce_one=True)\n",
    "print(\"write:\", result)\n",
    "\n",
    "# only the data file, not markers\n",
    "for f in dbutils.fs.ls(OUT_PATH):\n",
    "    if f.name.startswith(\"part-\"):\n",
    "        # print(f.name, f.size)\n",
    "        print(f.size)\n",
    "\n",
    "df_check = spark.read.parquet(OUT_PATH)\n",
    "print(\"rows:\", df_check.count())\n",
    "df_check.show(3, truncate=35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81a6a21d-6062-4a00-9d04-1c7edab5b533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transform_question: string (nullable = true)\n",
      " |-- answer: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- focus_area: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_check.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\r\n",
    "\r\n",
    "This notebook successfully loaded and cleaned the raw Medical FAQ dataset (`medquad.csv`, 16,412 rows) from Azure Blob Storage, applied basic transformations, and saved it as `transformed_medquad.parquet` (16,359 rows) for efficient downstream processing in the Healthcare FAQ Generator pipeline.\r\n",
    "\r\n",
    "### Key Results\r\n",
    "- **Row Count**: 16,359 after cleaning (from 16,412 raw).\r\n",
    "- **Schema**: `transform_question` (lowercased, cleaned), `answer`, `source`, `focus_area` (all strings).\r\n",
    "- **Runtime**: ~1-2 minuAzure tes on DatEdition (CPU).\r\n",
    "- **Business Impact**: Establishes a clean, scalable data foundation for multilingual FAQ generation, supporting telehealth applications with reliable medical knowledge.\r\n",
    "\r\n",
    "### Next Steps\r\n",
    "- **Preprocessing**: Apply lemmatization and tokenization in the next notebook (02_preprocess_medquad.ipynb).\r\n",
    "- **Model Fine-Tuning**: Use the Parquet dataset to fine-tune `flan-t5-base` for FAQ generation.\r\n",
    "- **Evaluation**: Build RAG pipeline and evaluate with TF-IDF similarity.\r\n",
    "- **Portfolio Polish**: Document data lineage and schema in GitHub README.\r\n",
    "\r\n",
    "This step demonstrates PySpark ETL skills, Azure integration, and data engineering best practices for healthcare data pipelines."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_load_clean_data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python (projectML)",
   "language": "python",
   "name": "projectml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
